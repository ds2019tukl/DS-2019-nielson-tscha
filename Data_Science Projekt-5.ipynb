{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science: Exercise\n",
    "\n",
    "# Exercise Goals:\n",
    "* Learn a systematic approach to deal with a high level business request\n",
    "* Learn a data driven approach to ask and answer the correct questions\n",
    "* Learn a systematic approach to derive a machine learning model\n",
    "\n",
    "Procedure:\n",
    "* Exercise 1: Understand the data, visual explanation\n",
    "* Exercise 1: Define a possible business case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set:\n",
    "\n",
    "Historical of Gasoline Prices in Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import missingno as ms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "import string\n",
    "import geopandas as gpd\n",
    "from pandas import testing as pdt\n",
    "import unittest\n",
    "#configuring output\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#reading data from file and defining separator as ;\n",
    "data_station = pd.read_csv(\"example_sprit_cut_station.csv\", sep =';')\n",
    "data_prices = pd.read_csv(\"example_sprit_cut_prices.csv\", sep =';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after checking the data, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that we have duplicated data for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data_prices,data_station,left_on='STID',right_on= 'ID',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete row poluting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pivot_table function is going to be used to confirm the hypothesis that the cells are duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data_prices,index=[\"STID\",\"DATE_CHANGED\"],aggfunc=[len]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this result, we conclude that for every ID and DATE, there are 2 prices, which is clearly duplicated data. We are going to use the function drop_duplicates do solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.drop_duplicates(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the duplicates removed, now we have a half sized data frame to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the DATE_CHANGED column into optimized pandas datetime function, so later we can extract information from it in easier ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DATE_CHANGED'] =pd.to_datetime(data.DATE_CHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['ID'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, columns with useful extracted information are being created, in order to make the following work easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['YEAR'] = data.DATE_CHANGED.dt.year #This all could be a function\n",
    "data['DAY'] =data.DATE_CHANGED.dt.dayofyear\n",
    "data['WEEK'] =data.DATE_CHANGED.dt.weekofyear\n",
    "data['MONTH'] =data.DATE_CHANGED.dt.month\n",
    "data['HOUR'] =data.DATE_CHANGED.dt.hour\n",
    "data['WEEKDAY'] =data.DATE_CHANGED.dt.dayofweek\n",
    "data['WEEKDAYSTR']= data.DATE_CHANGED.dt.dayofweek\n",
    "data['WEEKDAYSTR'] = data['WEEKDAYSTR'].apply(lambda sym: {0:'SUN',1:'MON','2':'TUE',3:'WED',4:'THU',5:'FRI',6:'SAT'}.get(sym, ' '))\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling in empty cells in 'BRAND'\n",
    "data.loc[data.BRAND.isna(), 'BRAND'] = data['NAME']\n",
    "data.loc[data.BRAND == '\\\\N','BRAND'] = data['NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data,index=[\"BRAND\",\"YEAR\"],values = ['E10','E5','DIESEL'],aggfunc=[np.mean,len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this Pivot, we realized that our data needs more cleansing before we work with it\n",
    "\n",
    "First we're going to start with setting values of the 'NAMES' column for the Empty values of 'BRANDS' colum, and then standardizing the names of known brands and setting values of the names for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizing brand names\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'AUTO ZOTZ' if('AUTO ZOTZ' in str(x)) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'BFT' if(('Bft'in str(x))or ('bft'in str(x))) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'Freie Tankstelle' if('frei' in str(x)) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'TOTAL' if('Total' in str(x)) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'Eberhardt' if('Eberhardt' in str(x)) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'Globus' if('Globus' in str(x)) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'SBK' if('SBK' in str(x)) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'SB' if('SB' in str(x)and 'K'not in str(x)) else x)\n",
    "data['BRAND'] = data['BRAND'].apply(lambda x: 'Supermarkt' if('Supermarkt' in str(x)) else x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look in the data and in the matrix above, shows that there are some clear inconsistencies in the data. As examples, we could list:\n",
    "* Prices with values 0 or -1\n",
    "* HOUSENUMBER, STREET, POSTCODE can all be found, if necessary, using LAT and LON coordinates and reverse geocoding, we are going to join the main information in NAME to avoid double names\n",
    "* Stations which are no more active\n",
    "* Version and Version_Time columns which have no meaning description on the source and changed doesn't match the source description\n",
    "* Public Holiday Identifier also has no use since it's defined by station\n",
    "\n",
    "These inconsistencies will be treated on the next lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condensing information to make name unique\n",
    "data['NAME'] = data['NAME'] + \" \"+ data['PLACE'] + \"\" + data['STREET'] + \" \" + data[\"HOUSE_NUMBER\"]\n",
    "\n",
    "\n",
    "#dropping non unsed columns\n",
    "data.drop(columns=['POST_CODE','PUBLIC_HOLIDAY_IDENTIFIER','VERSION','VERSION_TIME','CHANGED','HOUSE_NUMBER','STREET'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping price inconsistency lines\n",
    "data.drop(data[(data.E10<800)|(data.E10>1800)|(data.E5<800)|(data.E5>1800)|\\\n",
    "               (data.DIESEL<800)|(data.DIESEL>1800)].index, inplace=True)\n",
    "data['E10'] = data['E10'].apply(lambda x:  x/1000)\n",
    "data['E5'] = data['E5'].apply(lambda x:  x/1000)\n",
    "data['DIESEL'] = data['DIESEL'].apply(lambda x:  x/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping no more active stations\n",
    "data.drop(data[data.NAME == 'nicht mehr aktiv'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting wrong date of year assignments\n",
    "data.loc[(data.DAY >= 360) & (data.WEEK==1), 'WEEK'] = '53'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Standardized names of the Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data,index=[\"BRAND\",\"YEAR\"],values = ['E10','E5','DIESEL'],aggfunc=[np.mean,len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check how the data looks and observe that some BRANDS and, as consequence, STATIONS, have a really small amount of data (By checking the columns len), we can filter this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the null data\n",
    "ms.matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the remaining null lines\n",
    "null_data = data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply a rule to fill the empty place names and check if there are any other empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treating the Stations with null name\n",
    "data['PLACE'] = data['NAME'].apply(lambda x: 'Landau' if('Landau' in str(x)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null data again\n",
    "null_data = data[data.isnull().any(axis=1)]\n",
    "null_data.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby(['NAME','YEAR'])['DAY'].nunique()\n",
    "grouped = grouped.groupby(['NAME']).sum()\n",
    "print(grouped.count())\n",
    "grouped_brand = data.groupby(['BRAND','YEAR'])['DAY'].nunique()\n",
    "grouped_brand = grouped_brand.groupby(['BRAND']).sum()\n",
    "print(grouped_brand.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WEEK'] = data['WEEK'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_complete_data(series,number):\n",
    "    cut_value = series.mean() -series.std()*2\n",
    "    passed = []\n",
    "    count = 0\n",
    "    series = series.sort_values(ascending=False)\n",
    "    for index,data in series.items():\n",
    "        if data>cut_value:\n",
    "            passed.append(index)\n",
    "            count=count+1\n",
    "            if count==number:\n",
    "                return passed\n",
    "    return passed\n",
    "\n",
    "def select_least_complete_data(series,number):\n",
    "    cut_value = series.mean() -series.std()*2\n",
    "    passed = []\n",
    "    count = 0\n",
    "    series = series.sort_values(ascending=True)\n",
    "    for index,data in series.items():\n",
    "        if data<cut_value:\n",
    "            passed.append(index)\n",
    "            count=count+1\n",
    "            if count==number:\n",
    "                return passed\n",
    "    return passed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed = select_most_complete_data(grouped,30)\n",
    "failed = select_least_complete_data(grouped,5)\n",
    "\n",
    "\n",
    "data_best = data[data['NAME'].isin(passed)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "group_best = data_best.groupby(['NAME','YEAR'])['DAY'].nunique()\n",
    "group_best = group_best.groupby(['NAME']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testFunctions(unittest.TestCase):\n",
    "    def test_passed(self):\n",
    "        self.assertEqual(select_most_complete_data(grouped,30), passed)\n",
    "    def test_failed(self):\n",
    "        self.assertEqual(select_least_complete_data(grouped,5), failed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testobj = testFunctions()\n",
    "testobj.test_passed()\n",
    "testobj.test_failed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many different stations exist in the data set and what is the existing history in days (bar chart)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of different Stations in the database is\", grouped.count())\n",
    "print(\"The number of different Brands in the database is\", grouped_brand.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of different stations is too big, we're gonna display in bar Graph the 30 Stations with longer time history and display the general result in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_best.plot(kind='bar',figsize=(50,10),fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes clear that there are many stations with the maximum number of days of historical data (485) and just some outliers with a small amount of data, which are gonna be filtered before we start working with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the min, mean, max price for each gasoline type and station weekly (time series graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed = select_most_complete_data(grouped,10)# this will be shown for the 10 best datastations \n",
    "data_best =data[data['NAME'].isin(passed)]\n",
    "\n",
    "pivot_prices =pd.pivot_table(data_best[(data_best['YEAR']==2015 )],index=[\"WEEK\",\"NAME\"],values = ['E10'],aggfunc=[np.max,np.mean,np.min])\n",
    "\n",
    "pivot_prices.swaplevel(0,1)\n",
    "pivot_prices.swaplevel(1,0)\n",
    "pivot_prices.columns = ['_'.join(col) for col in pivot_prices.columns]\n",
    "pivot_prices = pivot_prices.unstack()\n",
    "pivot_prices.columns = ['_'.join(col) for col in pivot_prices.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph (database,valuecol,index1,index2):\n",
    "    \n",
    "    pivot_prices =pd.pivot_table(data_best[(database.YEAR==2015)],index=[index1,index2],values = [valuecol],aggfunc=[np.max,np.mean,np.min])\n",
    "\n",
    "\n",
    "    pivot_prices.swaplevel(0,1)\n",
    "    pivot_prices.swaplevel(1,0)\n",
    "    pivot_prices.columns = ['_'.join(col) for col in pivot_prices.columns]\n",
    "    pivot_prices = pivot_prices.unstack()\n",
    "    pivot_prices.columns = ['_'.join(col) for col in pivot_prices.columns]\n",
    "    \n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    pivot_prices.plot(figsize=(120,100),fontsize=60,linestyle='-', linewidth=5)\n",
    "    plt.legend(loc='upper right',fontsize = 50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(data_best,'E10','WEEK','NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strong seasonality can be observed from months 5 to 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(data_best,'E5','WEEK','NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same pattern repeats for the Gasoline E5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(data_best,'DIESEL','WEEK','NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Diesel graph, it can be observed that the seasonality is not that strong as for the gasoline graphs but there are still some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed = select_most_complete_data(grouped,450) #450 > total number of stations, just to filter\n",
    "data_best =data[data['NAME'].isin(passed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_prices =pd.pivot_table(data_best,index='WEEKDAY',values = ['E10'],aggfunc=[np.mean])\n",
    "\n",
    "\n",
    "pivot_prices = pivot_prices.unstack()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is cheapest station (in average) and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the average prices for Each station and ordering\n",
    "pivot_cheap = pd.pivot_table(data,index=['NAME'],values = ['DIESEL','E10','E5'],aggfunc=[np.mean])\n",
    "\n",
    "pivot_cheap.columns = ['_'.join(col) for col in pivot_cheap.columns]\n",
    "pivot_cheap.reset_index(0,inplace= True)\n",
    "\n",
    "pivot_cheap['General_Mean'] =(pivot_cheap.mean_DIESEL + pivot_cheap.mean_E10 + pivot_cheap.mean_E5)/3\n",
    "\n",
    "pivot_cheap.sort_values(by = 'General_Mean',ascending = True)\n",
    "\n",
    "pivot_cheap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The cheapest Station, considering the average prices of the 3 types of fuel along the whole database is', pivot_cheap.NAME.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_graph(database,indexcol,valuecol,ylim1,ylim2):\n",
    "        pivot_prices =pd.pivot_table(data_best,index=[indexcol],values = [valuecol],aggfunc=[np.mean])\n",
    "        pivot_prices = pivot_prices.unstack()\n",
    "        pivot_prices.reset_index()\n",
    "        sns.set()\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        pivot_prices.plot(kind = 'bar',figsize=(120,100),fontsize=100,ylim=(ylim1,ylim2),x=indexcol)\n",
    "        diff = round(pivot_prices.mean(),3) -round(pivot_prices.min(),3)\n",
    "        diff = round(diff,3)\n",
    "        plt.title((valuecol + ' Distribution of '+ valuecol+ \" AVG = \"\\\n",
    "                   +str(round(pivot_prices.mean(),3)) + \", MIN = \"\\\n",
    "                  +str(round(pivot_prices.min(),3))+ \", MAX = \"\\\n",
    "                  +str(round(pivot_prices.max(),3))+ \", AVG-MIN = \"\\\n",
    "                   +str(diff)),fontsize=150)\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. At which day of a weak is the price most likely the cheapest (week profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_graph(data_best,'WEEKDAY','E10',1.420,1.440)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_graph(data_best,'WEEKDAY','E5',1.445,1.465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_graph(data_best,'WEEKDAY','DIESEL',1.240,1.250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the price floats along the week, being more expensive on weekends than during the weekdays, this is true for all 3 types of fuel. Also The difference between the Average and the minimum is only 0.002 for all three types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. At which hour during a day is the price the cheapest in average (hour profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_graph(data_best,'HOUR','E10',1.300,1.600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_graph(data_best,'HOUR','E5',1.300,1.600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_graph(data_best,'HOUR','DIESEL',1.100,1.400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hour profile, it can be observed that in the interval between 20:00 and 05:00, the prices are higher, reaching the lowest prices  between 12:00 and 19:00.\n",
    "The difference here can reach values between 0.15 and 0.20 Euro per liter.\n",
    "It's also important to observe that the differente from the Average value and the minimum in 18h is 0,08 0,083 and 0,084 for E10, E5 and diesel respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How many different station locations are present in the data (visualize via a map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord =data[['LAT','LNG']]           \n",
    "            \n",
    "coord =pd.DataFrame.drop_duplicates(coord)\n",
    "\n",
    "lat = tuple(coord['LAT']) \n",
    "lon = tuple(coord['LNG'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The map creation code may cause the Kernel to Die"
    "from mpl_toolkits.basemap import Basemap\n",
    "plt.figure(figsize=(20,10))\n",
    "map=Basemap(projection=\"lcc\",resolution=\"l\",width=1E6,height=1E6,\n",
    "                             lon_0=9.9167,lat_0=51.5167,fix_aspect=False)\n",
    "map.drawcountries(zorder=1,color=\"black\", linewidth=0.5)\n",
    "map.shadedrelief()\n",
    "map.drawcoastlines(color=\"black\",linewidth=1.2)\n",
    "map.drawmapboundary()\n",
    "map.drawstates()\n",
    "\n",
    "map.scatter(lon,lat,2,marker=\"o\",latlon=True,cmap = 'Reds')\n",
    "plt.title(\"Stations Locations in Germany\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the data covers the center-west part of Germany, centered around the Pfalz region.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is the gas station which has most price data points, choose one and draw the time series for all 3 gasoline types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_len = data_best[['NAME','E10','E5','DIESEL']]\n",
    "pivot_len =pd.pivot_table(data_len,index=[\"NAME\"],values = ['E10','E5','DIESEL'],aggfunc=[len])\n",
    "pivot_len.columns = ['_'.join(col) for col in pivot_len.columns]\n",
    "pivot_len = pivot_len.sort_values(by='len_E10', ascending = False)\n",
    "pivot_len.drop(columns = ['len_E5','len_DIESEL'], inplace = True)\n",
    "pivot_len.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_station_data = data_best[data_best.NAME ==   pivot_len['NAME'].loc[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot time series of the station with most data points    \n",
    "sns.set_style(\"whitegrid\")\n",
    "    \n",
    "best_station_data.plot(x ='DATE_CHANGED',y = ['E10','E5','DIESEL'],figsize=(120,100),fontsize=60,linestyle='-', linewidth=5)\n",
    "plt.title('Fuel Prices from the station with most data Points',fontsize=150)\n",
    "plt.legend(loc='upper right',fontsize = 50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_station_data.plot(x ='DATE_CHANGED',y = 'E10',figsize=(120,100),fontsize=60,\n",
    "                       linestyle='-', linewidth=5)\n",
    "plt.title('E10 prices from the station with most data Points',fontsize=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_station_data.plot(x ='DATE_CHANGED',y = 'E5',figsize=(150,120),fontsize=60,\n",
    "                       linestyle='-', linewidth=5,color = 'orange')\n",
    "plt.title('E5 prices from the station with most data Points',fontsize=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_station_data.plot(x ='DATE_CHANGED',y = 'DIESEL',figsize=(120,100),fontsize=60,\n",
    "                       linestyle='-', linewidth=5,color = 'green')\n",
    "plt.title('Diesel prices from the station with most data Points',fontsize=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation of these Time series Graphs, the seasonality can be observed again, having a peak July(Summer), starting to go down in November(2014) or August(2015), until a local minimum in February(Winter). A higher variation can also be noticed between June and July(2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. At which hour during a day do we have the most price changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating columns with change information (error of 1 per station change can be ignored)\n",
    "data_best['E10_CHANGE']=data_best['E10'].diff()!=0\n",
    "data_best['E5_CHANGE']=data_best['E5'].diff()!=0\n",
    "data_best['DIESEL_CHANGE']=data_best['DIESEL'].diff()!=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_change_E10 = pd.pivot_table(data_best[data_best.E10_CHANGE == True],index=['HOUR','E10_CHANGE'],values = ['E10'],aggfunc=[len])\n",
    "pivot_change_E10 = pivot_change_E10.sort_values(by=('len','E10'), ascending = False)\n",
    "pivot_change_E10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_change_E5 = pd.pivot_table(data_best[data_best.E5_CHANGE == True],index=['HOUR','E5_CHANGE'],values = ['E5'],aggfunc=[len])\n",
    "pivot_change_E5 = pivot_change_E5.sort_values(by=('len','E5'), ascending = False)\n",
    "\n",
    "\n",
    "pivot_change_E5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_change_DIESEL = pd.pivot_table(data_best[data_best.DIESEL_CHANGE == True],index=['HOUR','DIESEL_CHANGE'],values = ['DIESEL'],aggfunc=[len])\n",
    "pivot_change_DIESEL = pivot_change_DIESEL.sort_values(by=('len','DIESEL'), ascending = False)\n",
    "pivot_change_DIESEL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about price changes in the 3 tables concludes that the time where it's most likely to have price changes is from 12 to 1 PM, followed by the interval between 6 to 10 in the morning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Select 20 gas stations having the longest time history and visualize the average price per month. Use heatmap and only the prices between 12:00-13:00 of e10 and diesel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed = select_most_complete_data(grouped,20)\n",
    "\n",
    "data_best = data[data['NAME'].isin(passed)]\n",
    "data_best = data_best[['NAME','E10','DIESEL','MONTH','HOUR']]\n",
    "data_hmap=data_best[(data_best.HOUR==12)]\n",
    "data_hmap.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_E10_mon = pd.pivot_table(data_best,index=[\"MONTH\",'NAME'],values = 'E10',aggfunc=[np.mean])\n",
    "pivot_E10_mon.swaplevel(0,1)\n",
    "pivot_E10_mon.swaplevel(1,0)\n",
    "pivot_E10_mon.columns = ['_'.join(col) for col in pivot_E10_mon.columns]\n",
    "pivot_E10_mon = pivot_E10_mon.unstack()\n",
    "pivot_E10_mon.columns = ['_'.join(col) for col in pivot_E10_mon.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,50))\n",
    "sns.set(font_scale=5)\n",
    "heat_map = sns.heatmap(pivot_E10_mon.transpose(),cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the heatmap, we can notice the seasonality, with the lighter colors in the \"Summer\" months and the darker color in the \"Winter\" Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_DIESEL_mon = pd.pivot_table(data_best,index=[\"MONTH\",'NAME'],values = 'DIESEL',aggfunc=[np.mean])\n",
    "pivot_DIESEL_mon.swaplevel(0,1)\n",
    "pivot_DIESEL_mon.swaplevel(1,0)\n",
    "pivot_DIESEL_mon.columns = ['_'.join(col) for col in pivot_DIESEL_mon.columns]\n",
    "pivot_DIESEL_mon = pivot_DIESEL_mon.unstack()\n",
    "pivot_DIESEL_mon.columns = ['_'.join(col) for col in pivot_DIESEL_mon.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,50))\n",
    "sns.set(font_scale=5)\n",
    "heat_map = sns.heatmap(pivot_DIESEL_mon.transpose(),cmap = 'magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same seasonality is observed for the Diesel, but the price for Diesel remains high until november, for the selected stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Describe a possible business potential in € for the customer (textual description in the ipyhton file). Define the constraints of the business case 5 lines, the answer max 15 lines (high level summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business case development\n",
    "\n",
    "\n",
    "### Assumptions:\n",
    "\n",
    "According to all the information extracted from the Data, we will give the following assumptions to be able to develop the business case:\n",
    "\n",
    "1. The Seasonality of the Work is fixed, therefore, it's not possible to fill tank more or less during the months\n",
    "\n",
    "2. The Cars can fill the Tanks in any of the stations in the region, since they will probably have been walking in a circular route in order to reduce the waste\n",
    "\n",
    "3. The cars run from Monday to Friday from 9 to 19 PM\n",
    "\n",
    "4. The Fleet consists of 1000 Cars that run on Diesel, each of the cars have to fill up their 60L tanks 3 times per week\n",
    "\n",
    "5. The fuel used by one car doesn't exceed one full tank per day\n",
    "\n",
    "6. Before, there were no stabilished rules, so we also assume that there were no trends for a definite time, weekday or gas station\n",
    "\n",
    "From the assumptions and observation of the information graphs, we conclude that the data that relies on monthly data wouldn't be so easily used to generate value. \n",
    "\n",
    "The difference based on days of the week has a small difference as well, as long as we forbit the fleet to fill up the tanks in the Weekends, but usually they don't work on weekends, but it could be a further suggestion, for now we decide to work with the hour distribution.\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "The conclusion with the data, would be stabilishing a recommendation that every Fleet Car should fill up the tank every day, between 18 and 19 PM, at the last working hour.\n",
    "\n",
    "\n",
    "### Calculation of the Benefit\n",
    "\n",
    "\n",
    "* Total Fuel Consumption per Week = 60Lx3x1000 = 180000 L of Diesel\n",
    "\n",
    "* Average Price Considering uniform probability distribution over hours: 1,269€\n",
    "\n",
    "* Expenditure of: <font color=red> __228.240€__   </font> per Week or <font color=red> __11.868.480€__</font> per Year\n",
    "\n",
    "* Average Price Considering recommendation: 1,185 €\n",
    "\n",
    "* Expenditure of: 213.300 € per Week\n",
    "\n",
    "* Weekly Economy: <font color=green> __15.120€__</font>\n",
    "\n",
    "* Anualized Economy Considering 52 Weeks:<font color=green> __786.240€__</font> \n",
    "\n",
    "* Investment required: <font color=green> __000000€__</font> \n",
    "\n",
    "\n",
    "### Decision/Recommendation\n",
    "Determine a rule that every employee should fill up the tank at the end of the working hours. The employee should be encouraged to do so, and the expense management system will also check whether that is happening or not.\n",
    "    The rule may be attached to some semester bonus the company may have and/or be a personal goal for each employees. \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
